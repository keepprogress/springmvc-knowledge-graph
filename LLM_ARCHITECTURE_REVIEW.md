# è§£æ”¾LLMæ€ç¶­ - æ¶æ§‹è¨­è¨ˆå¯©è¦–

**Date**: 2025-10-05
**Philosophy**: LLM-First vs Code-First Balance

---

## æ ¸å¿ƒå•é¡Œé™³è¿°

ç•¶å‰å°ˆæ¡ˆé¢è‡¨å…©å€‹æ¥µç«¯ï¼š

âŒ **æ¥µç«¯1: å®Œå…¨Prompté©…å‹•**
- ç„¡æ–¹æ³•è«–ï¼Œæµªè²»token
- æˆæœ¬ä¸å¯æ§
- ç²¾åº¦ç„¡ä¿è­‰

âŒ **æ¥µç«¯2: å®Œå…¨Codeé©…å‹•ï¼ˆç•¶å‰ç‹€æ…‹ï¼‰**
- éåº¦å·¥ç¨‹åŒ–
- ç„¡æ³•éš¨LLMé€²åŒ–
- ç¶­è­·æˆæœ¬é«˜

âœ… **ç†æƒ³ç‹€æ…‹: æ™ºèƒ½å¹³è¡¡**
- LLMè™•ç†è¤‡é›œèªæ„ï¼ŒCodeè™•ç†ç¢ºå®šé‚è¼¯
- å¯éš¨æ¨¡å‹èƒ½åŠ›æå‡è€Œæå‡
- æˆæœ¬å¯æ§ï¼Œç²¾åº¦å¯ä¿è­‰

---

## ç•¶å‰æ¶æ§‹çš„ã€ŒåLLMã€å‚¾å‘åˆ†æ

### Phase 3: ç¨‹å¼ç¢¼çµæ§‹æå– âš ï¸ éåº¦å·¥ç¨‹åŒ–

**ç•¶å‰è¨­è¨ˆï¼ˆCode-Firstï¼‰**:
```python
# JSP Analyzer - lxml parsing
- ç”¨lxmlè§£æHTML
- ç”¨regexæå–AJAX calls
- hardcodedæ‰€æœ‰å¯èƒ½çš„patterns

# Controller Analyzer - tree-sitter-java
- å¯«è¤‡é›œçš„tree-sitter queries
- æå–@RequestMapping
- æ‰‹å‹•è¿½è¹¤method calls

# Service Analyzer - tree-sitter-java
- é¡ä¼¼çš„tree-sitteré‚è¼¯
- æ‰‹å‹•è¿½è¹¤@Autowired

# MyBatis Analyzer - lxml + sqlparse
- XML parsing
- SQL parsing
- Table extraction
```

**å•é¡Œ**:
1. âŒ **å¤§é‡ç¶­è­·æˆæœ¬** - æ¯å€‹æ–°annotationéƒ½è¦æ”¹code
2. âŒ **ç„¡æ³•è™•ç†edge cases** - regex/parserç„¡æ³•ç†è§£èªæ„
3. âŒ **ä¸éš¨LLMé€²åŒ–** - Claude Opus 4å‡ºä¾†ï¼Œæˆ‘å€‘çš„å·¥å…·ä¸æœƒè®Šå¼·
4. âŒ **éåº¦ç²¾ç¢ºä½†ä¸éˆæ´»** - 100%æº–ç¢ºä½†åªèƒ½è™•ç†å·²çŸ¥patterns

**LLM-Firstæ›¿ä»£æ–¹æ¡ˆ**:
```python
class LLMDrivenAnalyzer:
    """Let LLM do the heavy lifting"""

    async def analyze_any_file(self, file_path: str, file_type: str) -> Dict:
        """Universal analyzer - LLM figures out what to extract"""

        # 1. Read file (simple)
        code = Path(file_path).read_text()

        # 2. Ask LLM what's in this file (adaptive)
        prompt = f"""
        Analyze this {file_type} file and extract ALL relevant information.

        File: {file_path}

        Code:
        ```{file_type}
        {code[:5000]}  # Smart truncation, not hardcoded Â±15 lines
        ```

        Extract (let LLM decide what's relevant):
        - For Controllers: endpoints, dependencies, method calls
        - For Services: transactional boundaries, dependencies
        - For JSP: includes, AJAX, forms, any UI interactions
        - For MyBatis: SQL, tables, procedures, parameters

        If you're unsure about something, ask for more context.
        Output structured JSON with confidence scores.
        """

        result = await self.llm.query(prompt)

        # 3. Validate critical parts only (not everything)
        if result.confidence < 0.7:
            # Use code parsing as fallback/validation
            code_result = await self.code_parser.parse(file_path, file_type)
            return self.merge_results(result, code_result)

        return result
```

**å„ªå‹¢**:
1. âœ… **ä¸€å€‹analyzerè™•ç†æ‰€æœ‰æ–‡ä»¶é¡å‹**
2. âœ… **è‡ªå‹•é©æ‡‰æ–°æ¡†æ¶**ï¼ˆSpringBoot 3.xæ–°è¨»è§£è‡ªå‹•è­˜åˆ¥ï¼‰
3. âœ… **éš¨LLMèƒ½åŠ›æå‡** - æ¨¡å‹è®Šå¼·ï¼Œåˆ†æå°±è®Šå¼·
4. âœ… **è™•ç†edge cases** - LLMç†è§£èªæ„ï¼Œä¸åªæ˜¯pattern matching

---

### Phase 5.1 vs 5.2: ä¸»æ¬¡é¡›å€’ âš ï¸

**ç•¶å‰è¨­è¨ˆ**:
```
Phase 5.1 (ä¸»): Code-based Graph (100% certain)
Phase 5.2 (è¼”): LLM-based Graph (gap filling)
```

**å•é¡Œåˆ†æ**:

| Aspect | Code-First (5.1ä¸») | LLM-First (5.2ä¸») |
|--------|-------------------|-------------------|
| **éˆæ´»æ€§** | âŒ åªèƒ½è™•ç†å·²çŸ¥patterns | âœ… é©æ‡‰ä»»ä½•code style |
| **å®Œæ•´æ€§** | âš ï¸ æœƒæ¼æ‰edge cases | âœ… èªæ„ç†è§£ï¼Œå¾ˆé›£æ¼ |
| **ç¶­è­·æˆæœ¬** | âŒ æ¯å€‹æ–°featureéƒ½è¦æ”¹code | âœ… æ”¹promptå³å¯ |
| **é€²åŒ–èƒ½åŠ›** | âŒ å›ºå®šèƒ½åŠ› | âœ… éš¨æ¨¡å‹æå‡ |
| **æˆæœ¬** | âœ… å…è²» | âš ï¸ éœ€è¦æ§åˆ¶ |

**LLM-Firstæ›¿ä»£è¨­è¨ˆ**:
```python
# Phase 5 Redesign: LLMä¸»å°ï¼ŒCodeé©—è­‰

class LLMFirstGraphBuilder:

    async def build_graph(self, project_dir: str) -> nx.DiGraph:
        """LLMä¸»å°å»ºåœ–ï¼ŒCodeè¼”åŠ©é©—è­‰"""

        # Step 1: è®“LLMç†è§£æ•´å€‹å°ˆæ¡ˆçµæ§‹ï¼ˆAdaptive Contextï¼‰
        project_structure = await self.understand_project(project_dir)

        # Step 2: LLMè‡ªå·±æ±ºå®šåˆ†æç­–ç•¥
        analysis_plan = await self.llm.plan_analysis(project_structure)

        # Step 3: LLMåŸ·è¡Œåˆ†æï¼ˆå¸¶cacheï¼‰
        for task in analysis_plan.tasks:
            llm_result = await self.llm.analyze(task)

            # Step 4: Codeé©—è­‰é—œéµéƒ¨åˆ†ï¼ˆä¸æ˜¯å…¨éƒ¨ï¼‰
            if task.is_critical:
                code_validation = await self.validate_with_code(llm_result)
                if code_validation.conflicts:
                    # LLMé‡æ–°åˆ†æï¼Œå¸¶ä¸Šè¡çªè³‡è¨Š
                    llm_result = await self.llm.reanalyze(
                        task,
                        validation_feedback=code_validation
                    )

            # Step 5: å»ºåœ–
            self.add_to_graph(llm_result)

        return self.graph
```

**æ ¸å¿ƒè½‰è®Š**:
- âŒ èˆŠ: Codeå»ºåœ– â†’ LLMè£œæ´
- âœ… æ–°: LLMå»ºåœ– â†’ Codeé©—è­‰é—œéµéƒ¨åˆ†

---

## è§£æ”¾LLMçš„æ–°æ¶æ§‹è¨­è¨ˆ

### 1. Prompt-First Pipeline â­

**è¨­è¨ˆç†å¿µ**: èƒ½ç”¨Promptè§£æ±ºçš„ï¼Œä¸å¯«Code

```python
class PromptFirstPipeline:
    """
    Pipeline: Prompt â†’ (optional) Code Validation â†’ Result
    """

    def __init__(self):
        self.llm = AdaptiveLLM()  # è‡ªé©æ‡‰model selection
        self.code_validator = LightweightValidator()  # è¼•é‡é©—è­‰

    async def analyze(self, input_data: Any) -> Result:
        # 1. Promptå˜—è©¦ï¼ˆä¸»åŠ›ï¼‰
        llm_result = await self.prompt_analysis(input_data)

        # 2. è©•ä¼°æ˜¯å¦éœ€è¦code validationï¼ˆè€Œä¸æ˜¯é è¨­éƒ½é©—è­‰ï¼‰
        if self.needs_validation(llm_result):
            validation = await self.code_validator.validate(llm_result)

            if validation.failed:
                # 3. Prompté‡è©¦ï¼Œå¸¶validation feedback
                llm_result = await self.prompt_analysis(
                    input_data,
                    previous_attempt=llm_result,
                    validation_errors=validation.errors
                )

        return llm_result

    def needs_validation(self, result: Result) -> bool:
        """æ™ºèƒ½åˆ¤æ–·æ˜¯å¦éœ€è¦codeé©—è­‰"""
        return (
            result.confidence < 0.8 or  # ä¿¡å¿ƒä¸è¶³
            result.involves_critical_logic or  # é—œéµé‚è¼¯
            result.has_security_implications  # å®‰å…¨ç›¸é—œ
        )
```

**å„ªå‹¢**:
- âœ… Promptç‚ºä¸»ï¼ŒCodeç‚ºè¼”
- âœ… åªåœ¨å¿…è¦æ™‚ç”¨Code
- âœ… è‡ªå‹•å¹³è¡¡æˆæœ¬èˆ‡ç²¾åº¦

---

### 2. Adaptive Context Window â­

**å•é¡Œ**: ç•¶å‰hardcoded Â±15è¡Œcontext

**æ›´å¥½çš„æ–¹å¼**: è®“LLMè‡ªå·±æ±ºå®šéœ€è¦å¤šå°‘context

```python
class AdaptiveContextExtractor:
    """LLMè‡ªé©æ‡‰æ±ºå®šcontextå¤§å°"""

    async def get_context_for_analysis(
        self,
        file_path: str,
        target_line: int,
        analysis_type: str
    ) -> str:
        """å‹•æ…‹æ±ºå®šcontext window"""

        # Step 1: å…ˆç”¨å°contextå•LLM
        initial_context = self.get_lines(file_path, target_line, window=5)

        initial_prompt = f"""
        åˆ†æé€™æ®µä»£ç¢¼ï¼ˆç¬¬{target_line}è¡Œï¼‰ã€‚

        Context:
        {initial_context}

        è«‹å‘Šè¨´æˆ‘ï¼š
        1. é€™æ®µä»£ç¢¼çš„åŠŸèƒ½æ˜¯ä»€éº¼ï¼Ÿ
        2. ä½ éœ€è¦æ›´å¤šcontextå—ï¼Ÿå¦‚æœéœ€è¦ï¼Œéœ€è¦çœ‹å“ªäº›éƒ¨åˆ†ï¼Ÿ
           - ä¸Šä¸‹æ–‡è¡Œæ•¸ï¼Ÿ
           - ç›¸é—œé¡åˆ¥/æ–¹æ³•ï¼Ÿ
           - Import statementsï¼Ÿ
        """

        initial_response = await self.llm.query(initial_prompt)

        # Step 2: æ ¹æ“šLLMè¦æ±‚æ“´å±•context
        if initial_response.needs_more_context:
            expanded_context = self.expand_context(
                file_path,
                initial_response.requested_context
            )

            final_prompt = f"""
            ä¹‹å‰çš„åˆ†æï¼š
            {initial_response.analysis}

            é¡å¤–contextï¼š
            {expanded_context}

            ç¾åœ¨å®Œæˆåˆ†æã€‚
            """

            return await self.llm.query(final_prompt)

        return initial_response
```

**å„ªå‹¢**:
- âœ… ä¸æµªè²»tokenï¼ˆä¸éœ€è¦æ™‚ä¸çµ¦å¤šé¤˜contextï¼‰
- âœ… ä¸éºæ¼è³‡è¨Šï¼ˆéœ€è¦æ™‚ä¸»å‹•è¦æ±‚ï¼‰
- âœ… é©æ‡‰ä¸åŒè¤‡é›œåº¦çš„ä»£ç¢¼

---

### 3. Agent-Based Analysis â­â­â­

**æ ¸å¿ƒæ€æƒ³**: è®“LLMä½œç‚ºAgentï¼Œè‡ªå·±è¦åŠƒåˆ†æç­–ç•¥

```python
class AnalysisAgent:
    """LLM Agent - è‡ªä¸»è¦åŠƒå’ŒåŸ·è¡Œåˆ†æ"""

    async def analyze_project(self, project_dir: str) -> KnowledgeGraph:
        """Agentè‡ªä¸»åˆ†æå°ˆæ¡ˆ"""

        # 1. Agentç†è§£å°ˆæ¡ˆçµæ§‹
        understanding = await self.understand_project_structure(project_dir)

        # 2. Agentè‡ªå·±åˆ¶å®šåˆ†æè¨ˆç•«
        plan = await self.create_analysis_plan(understanding)
        # plan = {
        #   "strategy": "bottom-up",  # Agentè‡ªå·±æ±ºå®šç­–ç•¥
        #   "steps": [
        #       "å…ˆåˆ†æDB schemaï¼ˆåŸºç¤ï¼‰",
        #       "å†åˆ†æMapperï¼ˆæ•¸æ“šå±¤ï¼‰",
        #       "ç„¶å¾ŒServiceï¼ˆæ¥­å‹™å±¤ï¼‰",
        #       "æœ€å¾ŒControllerå’ŒJSPï¼ˆå±•ç¤ºå±¤ï¼‰"
        #   ],
        #   "tools_needed": ["db_query", "file_read", "ast_parse"]
        # }

        # 3. AgentåŸ·è¡Œè¨ˆç•«ï¼ˆå¯èª¿æ•´ï¼‰
        for step in plan.steps:
            result = await self.execute_step(step, plan.tools_needed)

            # Agentè‡ªæˆ‘è©•ä¼°
            if not result.is_satisfactory:
                # Agentèª¿æ•´è¨ˆç•«
                plan = await self.adjust_plan(plan, result.issues)

        # 4. Agentå»ºæ§‹çŸ¥è­˜åœ–è­œ
        graph = await self.build_knowledge_graph(
            self.analysis_results,
            strategy=plan.graph_building_strategy  # Agentæ±ºå®šå»ºåœ–ç­–ç•¥
        )

        return graph

    async def execute_step(self, step: str, tools: List[str]) -> Result:
        """Agentä½¿ç”¨å·¥å…·åŸ·è¡Œæ­¥é©Ÿ"""

        # Agentè‡ªå·±æ±ºå®šç”¨ä»€éº¼å·¥å…·
        tool_choice = await self.choose_tool(step, tools)

        if tool_choice == "llm_only":
            return await self.llm_analysis(step)
        elif tool_choice == "code_parser":
            return await self.code_parsing(step)
        elif tool_choice == "hybrid":
            # Agentè‡ªå·±æ±ºå®šæ··åˆç­–ç•¥
            return await self.hybrid_analysis(step)
```

**å„ªå‹¢**:
- âœ… **å®Œå…¨è‡ªä¸»** - ä¸éœ€è¦é å…ˆå®šç¾©æ‰€æœ‰æ­¥é©Ÿ
- âœ… **è‡ªæˆ‘èª¿æ•´** - é‡åˆ°å•é¡Œè‡ªå‹•èª¿æ•´ç­–ç•¥
- âœ… **å·¥å…·é¸æ“‡** - è‡ªå·±æ±ºå®šç”¨LLMé‚„æ˜¯Code Parser
- âœ… **éš¨æ¨¡å‹é€²åŒ–** - æ›´å¼·çš„æ¨¡å‹ = æ›´å¥½çš„ç­–ç•¥

---

### 4. Hierarchical Model Strategy â­

**å•é¡Œ**: ç•¶å‰æ²’æœ‰model selectionç­–ç•¥

**æ”¹é€²**: æ™ºèƒ½è·¯ç”±ï¼Œç”¨å°çš„modelåšå°çš„äº‹

```python
class HierarchicalModelRouter:
    """åˆ†å±¤æ¨¡å‹ç­–ç•¥ - ä¾¿å®œmodel screeningï¼Œè²´modelæ·±åº¦åˆ†æ"""

    MODELS = {
        "haiku": {
            "cost": 0.25,  # per 1M tokens input
            "speed": "fast",
            "use_for": ["screening", "simple_extraction", "classification"]
        },
        "sonnet": {
            "cost": 3.0,
            "speed": "medium",
            "use_for": ["reasoning", "complex_analysis", "verification"]
        },
        "opus": {
            "cost": 15.0,
            "speed": "slow",
            "use_for": ["critical_decisions", "complex_reasoning", "edge_cases"]
        }
    }

    async def analyze_with_optimal_model(
        self,
        task: AnalysisTask
    ) -> Result:
        """æ™ºèƒ½é¸æ“‡model"""

        # Step 1: Haiku screening
        haiku_result = await self.query_haiku(f"""
        Quick analysis: {task.description}

        Questions:
        1. Is this task simple or complex?
        2. Confidence level (0-1)?
        3. Do you need a more powerful model?
        """)

        # Step 2: æ ¹æ“šHaikuåˆ¤æ–·ï¼Œæ±ºå®šæ˜¯å¦å‡ç´š
        if haiku_result.is_simple and haiku_result.confidence > 0.9:
            return haiku_result  # Haikuå°±å¤ äº†ï¼ŒçœéŒ¢

        # Step 3: Sonnetæ·±åº¦åˆ†æ
        sonnet_result = await self.query_sonnet(f"""
        Haikuçš„åˆæ­¥åˆ†æï¼š
        {haiku_result.analysis}

        è«‹æ·±åº¦åˆ†æï¼š
        {task.description}

        Haikuèªç‚ºéœ€è¦æ³¨æ„ï¼š{haiku_result.concerns}
        """)

        if sonnet_result.confidence > 0.85:
            return sonnet_result

        # Step 4: æ¥µç«¯æƒ…æ³ç”¨Opus
        if task.is_critical or sonnet_result.has_edge_cases:
            opus_result = await self.query_opus(f"""
            ä¹‹å‰çš„åˆ†æï¼š
            - Haiku: {haiku_result.summary}
            - Sonnet: {sonnet_result.summary}

            é—œéµå•é¡Œï¼š
            {sonnet_result.critical_issues}

            è«‹æœ€çµ‚åˆ¤æ–·ã€‚
            """)
            return opus_result

        return sonnet_result
```

**æˆæœ¬å°æ¯”**:
```
æƒ…å¢ƒï¼šåˆ†æ100å€‹Controller

æ–¹æ¡ˆ1ï¼ˆç•¶å‰ - å…¨ç”¨Sonnetï¼‰:
- 100 files Ã— 2000 tokens Ã— $3/1M = $0.60

æ–¹æ¡ˆ2ï¼ˆæ™ºèƒ½è·¯ç”±ï¼‰:
- 80 files â†’ Haiku screening â†’ simple â†’ $0.04
- 15 files â†’ Sonnetæ·±åº¦åˆ†æ â†’ $0.09
- 5 files â†’ Opusé—œéµæ±ºç­– â†’ $0.15
- Total: $0.28 (çœäº†53%)

ä¸”ç²¾åº¦æ›´é«˜ï¼ˆé—œéµéƒ¨åˆ†ç”¨Opusï¼‰
```

---

### 5. Self-Improving Prompt Library â­

**å•é¡Œ**: ç•¶å‰promptsæ˜¯staticçš„

**æ”¹é€²**: Promptså¾ç¶“é©—ä¸­å­¸ç¿’

```python
class SelfImprovingPromptLibrary:
    """è‡ªæˆ‘æ”¹é€²çš„Promptåº«"""

    def __init__(self):
        self.prompt_templates = {}
        self.success_examples = {}  # æˆåŠŸæ¡ˆä¾‹
        self.failure_patterns = {}   # å¤±æ•—æ¨¡å¼

    async def get_prompt_for_task(
        self,
        task_type: str,
        context: Dict
    ) -> str:
        """å–å¾—å„ªåŒ–éçš„prompt"""

        # 1. åŸºç¤template
        base_template = self.prompt_templates[task_type]

        # 2. åŠ å…¥æˆåŠŸæ¡ˆä¾‹ï¼ˆfew-shot learningï¼‰
        few_shot = self.get_relevant_examples(task_type, context)

        # 3. åŠ å…¥å¤±æ•—æ•™è¨“
        warnings = self.get_failure_warnings(task_type, context)

        # 4. çµ„åˆæœ€å„ªprompt
        optimized_prompt = f"""
        {base_template}

        åƒè€ƒé€™äº›æˆåŠŸæ¡ˆä¾‹ï¼š
        {few_shot}

        æ³¨æ„é¿å…é€™äº›éŒ¯èª¤ï¼š
        {warnings}

        ç¾åœ¨åˆ†æï¼š
        {context}
        """

        return optimized_prompt

    def learn_from_result(
        self,
        task_type: str,
        prompt: str,
        result: Result,
        validation: Validation
    ):
        """å¾çµæœä¸­å­¸ç¿’"""

        if validation.is_success:
            # æˆåŠŸ - åŠ å…¥æˆåŠŸæ¡ˆä¾‹åº«
            self.success_examples[task_type].append({
                "prompt": prompt,
                "result": result,
                "context": result.context,
                "confidence": result.confidence
            })
        else:
            # å¤±æ•— - è¨˜éŒ„å¤±æ•—æ¨¡å¼
            self.failure_patterns[task_type].append({
                "prompt": prompt,
                "error": validation.error,
                "what_went_wrong": validation.analysis
            })

            # è‡ªå‹•æ”¹é€²prompt template
            self.improve_template(task_type, validation)

    def improve_template(self, task_type: str, validation: Validation):
        """è‡ªå‹•æ”¹é€²prompt template"""

        improvement_prompt = f"""
        é€™å€‹prompt templateæ•ˆæœä¸å¥½ï¼š
        {self.prompt_templates[task_type]}

        å¤±æ•—åŸå› ï¼š
        {validation.analysis}

        è«‹æ”¹é€²é€™å€‹templateï¼Œä½¿å…¶é¿å…é€™é¡éŒ¯èª¤ã€‚
        """

        improved = await self.llm.query(improvement_prompt)
        self.prompt_templates[task_type] = improved.new_template
```

**æ•ˆæœ**:
- âœ… Promptsè¶Šç”¨è¶Šå¥½
- âœ… è‡ªå‹•å­¸ç¿’edge cases
- âœ… å»ºç«‹é ˜åŸŸçŸ¥è­˜åº«

---

## æ–°æ¶æ§‹å®Œæ•´è¨­è¨ˆ

### æ•´é«”æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Analysis Agent (LLM-Driven)            â”‚
â”‚                                                   â”‚
â”‚  1. ç†è§£å°ˆæ¡ˆ â†’ 2. åˆ¶å®šè¨ˆç•« â†’ 3. åŸ·è¡Œåˆ†æ â†’ 4. å»ºåœ–  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prompt Engine    â”‚      â”‚  Tool Arsenal    â”‚
â”‚                   â”‚      â”‚                  â”‚
â”‚ â€¢ Adaptive Contextâ”‚      â”‚ â€¢ Code Parser    â”‚
â”‚ â€¢ Self-Improving  â”‚      â”‚ â€¢ DB Query       â”‚
â”‚ â€¢ Model Router    â”‚      â”‚ â€¢ AST Tools      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Validation Layer     â”‚
         â”‚                       â”‚
         â”‚ â€¢ Critical checks     â”‚
         â”‚ â€¢ Confidence scoring  â”‚
         â”‚ â€¢ Conflict resolution â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Knowledge Graph      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒè½‰è®Š

| Aspect | èˆŠè¨­è¨ˆ (Code-First) | æ–°è¨­è¨ˆ (LLM-First) |
|--------|--------------------|--------------------|
| **ä¸»å°è€…** | Code parsers | LLM Agent |
| **Codeè§’è‰²** | ä¸»åŠ›åˆ†æ | é©—è­‰å·¥å…· |
| **Context** | å›ºå®šÂ±15è¡Œ | è‡ªé©æ‡‰ |
| **ç­–ç•¥** | é å®šç¾©pipeline | Agentè‡ªä¸»æ±ºå®š |
| **Model** | å–®ä¸€Sonnet | åˆ†å±¤è·¯ç”±(Haikuâ†’Sonnetâ†’Opus) |
| **Prompts** | Static | Self-improving |
| **é€²åŒ–èƒ½åŠ›** | âŒ å›ºå®š | âœ… éš¨æ¨¡å‹æå‡ |
| **æˆæœ¬** | ä¸­ç­‰ | æ›´ä½ï¼ˆæ™ºèƒ½è·¯ç”±ï¼‰ |
| **ç²¾åº¦** | é«˜ä½†æœ‰é™ | æ›´é«˜ï¼ˆé—œéµè™•ç”¨Opusï¼‰ |

---

## å…·é«”å¯¦æ–½å»ºè­°

### Phase 3 é‡æ§‹å»ºè­°

**Option 1: æ¼¸é€²å¼ï¼ˆæ¨è–¦ï¼‰**
```python
# ä¿ç•™ç¾æœ‰parsersä½œç‚ºfallbackï¼ŒåŠ å…¥LLMå±¤

class HybridAnalyzer:
    def __init__(self):
        self.llm_analyzer = LLMDrivenAnalyzer()
        self.code_parser = TreeSitterParser()  # ç¾æœ‰çš„

    async def analyze(self, file_path: str) -> Result:
        # 1. å…ˆç”¨LLMï¼ˆå¿«é€Ÿï¼Œèªæ„ç†è§£ï¼‰
        llm_result = await self.llm_analyzer.analyze(file_path)

        # 2. åªåœ¨confidenceä½æ™‚ç”¨code parseré©—è­‰
        if llm_result.confidence < 0.8:
            code_result = self.code_parser.parse(file_path)
            return self.merge(llm_result, code_result)

        return llm_result
```

**Option 2: æ¿€é€²å¼**
```python
# å®Œå…¨ç”¨Agentï¼Œcode parserè®Šæˆtool

class AgentDrivenAnalyzer:
    async def analyze(self, file_path: str) -> Result:
        return await self.agent.analyze(
            task=f"åˆ†æ{file_path}",
            tools=["llm", "tree_sitter", "regex", "ast"],
            let_agent_decide=True  # Agentè‡ªå·±æ±ºå®šç”¨ä»€éº¼å·¥å…·
        )
```

### Phase 5 é‡æ§‹å»ºè­°

**é—œéµæ”¹è®Š**:
1. **5.1å’Œ5.2åˆä½µ** â†’ å–®ä¸€Agent-driven pipeline
2. **LLMä¸»å°** â†’ Codeé©—è­‰é—œéµç¯€é»
3. **è‡ªé©æ‡‰ç­–ç•¥** â†’ Agentæ±ºå®šå»ºåœ–é †åº

```python
class AgentGraphBuilder:
    async def build_graph(self, project_dir: str) -> nx.DiGraph:
        # Agentè‡ªä¸»åˆ†æå°ˆæ¡ˆ
        plan = await self.agent.understand_and_plan(project_dir)

        # AgentåŸ·è¡Œï¼ˆæ··åˆLLMå’ŒCodeï¼‰
        for step in plan.adaptive_steps:
            result = await self.agent.execute(
                step,
                prefer="llm",  # å„ªå…ˆç”¨LLM
                fallback="code"  # ä½ä¿¡å¿ƒæ™‚ç”¨code
            )
            self.graph.add(result)

        return self.graph
```

---

## æˆæœ¬èˆ‡æ•ˆç›Šåˆ†æ

### æˆæœ¬å°æ¯”ï¼ˆä¸­å‹å°ˆæ¡ˆ500K LOCï¼‰

**ç•¶å‰è¨­è¨ˆï¼ˆCode-Firstï¼‰**:
```
Phase 3: $0ï¼ˆcode parsingï¼‰
Phase 5.1: $0ï¼ˆcode parsingï¼‰
Phase 5.2: $40ï¼ˆLLM gap fillingï¼‰
Total: $40
Time: éœ€è¦å¯«å¤§é‡parsersï¼Œç¶­è­·æˆæœ¬é«˜
```

**æ–°è¨­è¨ˆï¼ˆLLM-First withæ™ºèƒ½è·¯ç”±ï¼‰**:
```
Phase 3 (Agent-driven):
- Haiku screening: 500 files Ã— $0.25/1M = $0.13
- Sonnetåˆ†æ: 100 files Ã— $3/1M = $0.30
- Opusé—œéµ: 20 files Ã— $15/1M = $0.30
Subtotal: $0.73

Phase 5 (Agentå»ºåœ–):
- Haikué—œä¿‚æ¨æ–·: $0.50
- Sonnetè¤‡é›œé—œä¿‚: $2.00
- Opusé—œéµæ±ºç­–: $1.00
Subtotal: $3.50

Total: $4.23

ä½†æ˜¯ï¼š
- âœ… ç¶­è­·æˆæœ¬å¤§å¹…é™ä½ï¼ˆä¸ç”¨ç¶­è­·è¤‡é›œparsersï¼‰
- âœ… ç²¾åº¦æ›´é«˜ï¼ˆOpusè™•ç†é—œéµéƒ¨åˆ†ï¼‰
- âœ… é©æ‡‰æ€§æ›´å¼·ï¼ˆæ–°æ¡†æ¶è‡ªå‹•æ”¯æ´ï¼‰
- âœ… éš¨æ¨¡å‹é€²åŒ–ï¼ˆClaude 4å‡ºä¾†è‡ªå‹•è®Šå¼·ï¼‰
```

### ROIåˆ†æ

**Code-Firstç¶­è­·æˆæœ¬ï¼ˆå¹´ï¼‰**:
- Parseræ›´æ–°: 40å°æ™‚ Ã— $100/å°æ™‚ = $4,000
- Bug fixes: 20å°æ™‚ Ã— $100/å°æ™‚ = $2,000
- æ–°feature: 60å°æ™‚ Ã— $100/å°æ™‚ = $6,000
- **Total: $12,000/å¹´**

**LLM-Firstç¶­è­·æˆæœ¬ï¼ˆå¹´ï¼‰**:
- Promptå„ªåŒ–: 10å°æ™‚ Ã— $100/å°æ™‚ = $1,000
- Modelå‡ç´š: 5å°æ™‚ Ã— $100/å°æ™‚ = $500
- ç›£æ§èª¿æ•´: 5å°æ™‚ Ã— $100/å°æ™‚ = $500
- **Total: $2,000/å¹´**

**ç¯€çœ: $10,000/å¹´** ğŸ’°

---

## å¯¦æ–½è·¯ç·šåœ–

### Week 1-2: Agentæ¡†æ¶å»ºè¨­
```python
# 1. å»ºç«‹AgentåŸºç¤è¨­æ–½
- AnalysisAgentåŸºé¡
- Tool registry (LLM, Code parsers, etc.)
- Adaptive context system
- Model router

# 2. é‡æ§‹ä¸€å€‹analyzerä½œç‚ºPOC
- é¸æ“‡Controller Analyzerï¼ˆæœ€è¤‡é›œçš„ï¼‰
- å¯¦ä½œAgent-drivenç‰ˆæœ¬
- å°æ¯”èˆŠç‰ˆæ•ˆæœ
```

### Week 3-4: Prompt Engineering
```python
# 3. Self-improving prompt library
- å»ºç«‹prompt templates
- Few-shot learningæ©Ÿåˆ¶
- å¤±æ•—æ¨¡å¼å­¸ç¿’

# 4. Hierarchical model strategy
- Haiku/Sonnet/Opusè·¯ç”±
- æˆæœ¬ç›£æ§
- æ•ˆæœè©•ä¼°
```

### Week 5-6: å…¨é¢æ‡‰ç”¨
```python
# 5. Phase 3 å…¨é¢AgentåŒ–
- æ‰€æœ‰analyzersç”¨Agenté‡å¯«
- Code parsersé™ç´šç‚ºtools

# 6. Phase 5 Agentå»ºåœ–
- çµ±ä¸€çš„Agent-driven graph building
- ç§»é™¤5.1/5.2åˆ†é›¢
```

---

## é¢¨éšªèˆ‡å°ç­–

### Risk 1: LLMä¸ç©©å®š
**å°ç­–**:
- Code parsersä½œç‚ºfallbackæ°¸é ä¿ç•™
- Critical pathså¼·åˆ¶ç”¨codeé©—è­‰

### Risk 2: æˆæœ¬è¶…æ”¯
**å°ç­–**:
- åš´æ ¼çš„æˆæœ¬ç›£æ§
- æ¯æ—¥budgeté™åˆ¶
- Haiku screeningå¼·åˆ¶åŸ·è¡Œ

### Risk 3: ç²¾åº¦ä¸‹é™
**å°ç­–**:
- å»ºç«‹gold-standard test set
- æ¯æ¬¡è®Šæ›´éƒ½regression test
- Confidence thresholdå¼·åˆ¶åŸ·è¡Œï¼ˆ<0.7å¿…é ˆcodeé©—è­‰ï¼‰

---

## çµè«–èˆ‡å»ºè­°

### âœ… å¼·çƒˆå»ºè­°æ¡ç”¨çš„æ”¹é€²

1. **Agent-Basedæ¶æ§‹** â­â­â­
   - æœ€ç¬¦åˆã€Œè§£æ”¾LLMã€ç†å¿µ
   - é•·æœŸç¶­è­·æˆæœ¬æœ€ä½
   - éš¨æ¨¡å‹é€²åŒ–

2. **Hierarchical Model Strategy** â­â­â­
   - ç«‹å³é™ä½æˆæœ¬50%+
   - ç²¾åº¦ä¸é™åå‡
   - å®¹æ˜“å¯¦æ–½

3. **Self-Improving Prompts** â­â­
   - è¶Šç”¨è¶Šå¥½
   - å»ºç«‹é ˜åŸŸçŸ¥è­˜
   - ä¸­æœŸå—ç›Š

4. **Adaptive Context** â­â­
   - ä¸æµªè²»token
   - ä¸éºæ¼è³‡è¨Š
   - å®¹æ˜“å¯¦æ–½

### âš ï¸ éœ€è¦è¬¹æ…è©•ä¼°

1. **å®Œå…¨ç§»é™¤Code Parsers**
   - å¤ªæ¿€é€²ï¼Œä¸å»ºè­°
   - ä¿ç•™ä½œç‚ºfallback/validation

2. **å®Œå…¨ä¾è³´LLM**
   - æˆæœ¬é¢¨éšª
   - ç©©å®šæ€§å•é¡Œ
   - éœ€è¦å¼·åŠ›ç›£æ§

### ğŸ“‹ Action Items

**Immediate (æœ¬é€±)**:
1. âœ… å»ºç«‹AgentåŸºç¤æ¡†æ¶
2. âœ… å¯¦ä½œModel Routerï¼ˆHaikuâ†’Sonnetâ†’Opusï¼‰
3. âœ… POC: ç”¨Agenté‡å¯«ä¸€å€‹analyzer

**Short-term (2-4é€±)**:
1. Self-improving prompt library
2. Adaptive context system
3. Phase 3 æ¼¸é€²å¼é‡æ§‹

**Long-term (1-2æœˆ)**:
1. å…¨é¢AgentåŒ–
2. å»ºç«‹çŸ¥è­˜åº«
3. æŒçºŒå„ªåŒ–

---

## å“²å­¸ç¸½çµ

**å¹³è¡¡ä¹‹é“**:

```python
# âŒ ä¸è¦é€™æ¨£ï¼ˆæ¥µç«¯Code-Firstï¼‰
if problem:
    write_complex_parser()
    maintain_forever()
    cant_evolve()

# âŒ ä¹Ÿä¸è¦é€™æ¨£ï¼ˆæ¥µç«¯Prompt-Firstï¼‰
if problem:
    throw_money_at_llm()
    waste_tokens()
    no_control()

# âœ… è¦é€™æ¨£ï¼ˆæ™ºèƒ½å¹³è¡¡ï¼‰
if problem:
    # 1. LLMå…ˆå˜—è©¦ï¼ˆä¸»åŠ›ï¼‰
    result = await llm.solve(problem)

    # 2. è©•ä¼°éœ€ä¸éœ€è¦code
    if result.needs_validation:
        validation = code.verify(result)

        if validation.failed:
            # 3. LLMé‡è©¦ï¼Œå¸¶feedback
            result = await llm.solve(
                problem,
                learned_from=validation
            )

    # 4. å­¸ç¿’æ”¹é€²
    system.learn(result)

    return result
```

**æ ¸å¿ƒåŸå‰‡**:
1. ğŸ¤– **LLMç‚ºä¸»**ï¼ŒCodeç‚ºè¼”
2. ğŸ’° **æˆæœ¬å¯æ§**ï¼Œæ™ºèƒ½è·¯ç”±
3. ğŸ¯ **ç²¾åº¦ä¿è­‰**ï¼Œé—œéµé©—è­‰
4. ğŸ“ˆ **æŒçºŒé€²åŒ–**ï¼Œè‡ªæˆ‘å­¸ç¿’
5. ğŸ›¡ï¸ **é¢¨éšªå¯æ§**ï¼Œå¤šé‡ä¿éšª

---

**Status**: Architectural Philosophy Document
**Next**: Implement Agent Framework POC
**Goal**: Balance LLM capability with practical constraints

ğŸ¤– Generated with Claude Code
